"""
This script performs the following:
- Generate synthetic training data to train the full and partial models
- Save the model checkpoints in the corresponding folders
- Calculate the test statistic on the inference sample
- Get predictive models for null features and save the checkpoints

Prerequsites:
- make sure the data folder is complete with info.json and names_dict.pkl and twin folders are created (run script_twin.py)
- in addition, exp folder should have synthetic data under both twin folders

Result folder structure:
- dataset_name
    - syn_pred_sample           # synthetic samples in numpy arrays, generated by tabular diffusion model
    - pred_model_ckpt           # checkpoints for the full and partial models for predicting y_feature
    - null_model_ckpt           # checkpoints for the null models for predicting null_features, by twin folders
        - twin_1_{suffix_null}
        - twin_2_{suffix_null}
"""

import os
import pickle
import json

import pandas as pd

import sys

from utils_syninf import *

device = "cuda:1"

# currently only support numerical null features
dataset_name = "adult_female_3000"


# null_features_list = ["age", "capital-gain"]
# null_features_list = ["age", "educationl-num"]
null_features_list = ["age", "educationl-num", "hours-per-week"]

# # the new fine-tuning pipeline might not work for the previous california experiment
# dataset_name = "california"
# null_features_list = ["MedInc"]

# dataset paths
dataset_dir = os.path.join(TDDPM_DIR, f"data/{dataset_name}")
dataset_dir_twin_1 = os.path.join(TDDPM_DIR, f"data/{dataset_name}_twin_1")
dataset_dir_twin_2 = os.path.join(TDDPM_DIR, f"data/{dataset_name}_twin_2")

# meta info, names dictionaries
data_info_dict = json.load(open(os.path.join(dataset_dir_twin_1, "info.json"), "rb"))
names_dict = pickle.load(open(os.path.join(dataset_dir, "names_dict.pkl"), "rb"))


num_samples = 5 * (2 * data_info_dict["train_size"])

# Generate training samples for getting the full model and the partial model
train_df_dict = {}
for twin_kw in ["twin_1", "twin_2"]:
    exp_folder = os.path.join(
        TDDPM_DIR, "exp", f"{dataset_name}_{twin_kw}", "ddpm_cb_best"
    )
    syn_sample_path = generate_sample(
        pipeline_config_path=os.path.join(exp_folder, "config.toml"),
        ckpt_path=os.path.join(exp_folder, "model.pt"),
        pipeline_dict_path=os.path.join(exp_folder, "pipeline_dict.joblib"),
        num_samples=num_samples,
        batch_size=num_samples,
        temp_parent_dir=os.path.join(
            SYNINF_DIR, dataset_name, f"syn_pred_sample/{twin_kw}"
        ),
        device=device,
    )
    train_df_dict[twin_kw] = concat_data(syn_sample_path, **names_dict)

# Prepare data as dataframes (2 * num_samples, *): first half for full model, second half for partial model
train_df = pd.concat([train_df_dict["twin_1"], train_df_dict["twin_2"]], axis=0)
train_df = train_df.sample(frac=1, random_state=2023)
train_df.to_csv(
    os.path.join(SYNINF_DIR, dataset_name, "syn_pred_sample/train_df_merged.csv")
)


# The entire inference sample: twin_1 train + twin_2 train + val
train_df_twin_1 = concat_data(dataset_dir_twin_1, **names_dict)
train_df_twin_2 = concat_data(dataset_dir_twin_2, **names_dict)
val_df = concat_data(dataset_dir_twin_1, split="val", **names_dict)

inf_df = pd.concat(
    [val_df, train_df_twin_1, train_df_twin_2], axis=0, ignore_index=True
)


# Prepare full and partial predictive models
ckpt_dir = os.path.join(SYNINF_DIR, dataset_name, "pred_model_ckpt")
if not os.path.exists(ckpt_dir):
    os.makedirs(ckpt_dir)

suffix = "_".join(null_features_list)
## train or load the full model
model_full_path = os.path.join(ckpt_dir, f"model_full_{suffix}.ckpt")
if not os.path.exists(model_full_path):
    print("Train the full model")
    model_full = catboost_pred_model(
        train_df[:num_samples], inf_df, **names_dict, null_features_list=[]
    )
    model_full.save_model(os.path.join(ckpt_dir, f"model_full_{suffix}.ckpt"))
else:
    print("Load existed checkpoint for the full model")
    model_full = CatBoostClassifier() if names_dict["is_y_cat"] else CatBoostRegressor()
    model_full.load_model(model_full_path)

## train or load the partial model
model_partial_path = os.path.join(ckpt_dir, f"model_partial_{suffix}.ckpt")
if not os.path.exists(model_partial_path):
    print("Train the partial model")
    model_partial = catboost_pred_model(
        train_df[num_samples:],
        inf_df,
        **names_dict,
        null_features_list=null_features_list,
    )
    model_partial.save_model(os.path.join(ckpt_dir, f"model_partial_{suffix}.ckpt"))
else:
    print("Load existed checkpoint for the partial model")
    model_partial = (
        CatBoostClassifier() if names_dict["is_y_cat"] else CatBoostRegressor()
    )
    model_partial.load_model(model_partial_path)


# Create predictive models for predicting the null features


ckpt_dir_twin_1 = os.path.join(
    SYNINF_DIR, dataset_name, "null_model_ckpt", f"twin_1_{suffix}"
)
if not os.path.exists(ckpt_dir_twin_1):
    os.makedirs(ckpt_dir_twin_1)

    print("Train the null models for twin_1")
    ckpt_dict_twin_1 = catboost_null_models(
        train_df_twin_1,
        # val_df,
        train_df_twin_2,
        **names_dict,
        null_feature_names=null_features_list,
        ckpt_dir=ckpt_dir_twin_1,
    )


ckpt_dir_twin_2 = os.path.join(
    SYNINF_DIR, dataset_name, "null_model_ckpt", f"twin_2_{suffix}"
)
if not os.path.exists(ckpt_dir_twin_2):
    os.makedirs(ckpt_dir_twin_2)

    print("Train the null models for twin_2")
    ckpt_dict_twin_2 = catboost_null_models(
        train_df_twin_2,
        # val_df,
        train_df_twin_1,
        **names_dict,
        null_feature_names=null_features_list,
        ckpt_dir=ckpt_dir_twin_2,
    )
